{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import uproot\n",
    "import pandas\n",
    "from functools import partial\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Dense, Conv2D, Dropout, AlphaDropout, Activation, BatchNormalization, Flatten, \\\n",
    "                                    Concatenate, PReLU, TimeDistributed, LSTM, Masking\n",
    "from keras.callbacks import Callback, ModelCheckpoint, CSVLogger\n",
    "#from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "sys.path.insert(0, \"../../python\")\n",
    "from common import *\n",
    "from DataLoader import DataLoader, read_hdf_lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafeModelCheckpoint(ModelCheckpoint):\n",
    "    def __init__(self, filepath, **kwargs):\n",
    "        super(SafeModelCheckpoint, self).__init__(filepath, **kwargs)\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        read_hdf_lock.acquire()\n",
    "        super(SafeModelCheckpoint, self).on_epoch_end(epoch, logs)\n",
    "        read_hdf_lock.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, learning_rate):\n",
    "    #opt = keras.optimizers.Adam(lr=learning_rate)\n",
    "    opt = keras.optimizers.Nadam(lr=learning_rate, schedule_decay=1e-4)\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "    metrics = [\n",
    "        \"accuracy\", TauLosses.tau_crossentropy, TauLosses.tau_crossentropy_v2,\n",
    "        TauLosses.Le, TauLosses.Lmu, TauLosses.Ljet,\n",
    "        TauLosses.He, TauLosses.Hmu, TauLosses.Htau, TauLosses.Hjet,\n",
    "        TauLosses.Hcat_e, TauLosses.Hcat_mu, TauLosses.Hcat_jet, TauLosses.Hbin,\n",
    "        TauLosses.Hcat_eInv, TauLosses.Hcat_muInv, TauLosses.Hcat_jetInv,\n",
    "        TauLosses.Fe, TauLosses.Fmu, TauLosses.Fjet, TauLosses.Fcmb\n",
    "    ]\n",
    "    model.compile(loss=TauLosses.tau_crossentropy_v2, optimizer=opt, metrics=metrics, weighted_metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_file(f_name):\n",
    "    file_objs = [ obj for obj in gc.get_objects() if (\"TextIOWrapper\" in str(type(obj))) and (obj.name == f_name)]\n",
    "    for obj in file_objs:\n",
    "        obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeCheckpoint(Callback):\n",
    "    def __init__(self, time_interval, file_name_prefix):\n",
    "        self.time_interval = time_interval\n",
    "        self.file_name_prefix = file_name_prefix\n",
    "        self.initial_time = time.time()\n",
    "        self.last_check_time = self.initial_time\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if self.time_interval is None or batch % 100 != 0: return\n",
    "        current_time = time.time()\n",
    "        delta_t = current_time - self.last_check_time\n",
    "        if delta_t >= self.time_interval:\n",
    "            abs_delta_t_h = (current_time - self.initial_time) / 60. / 60.\n",
    "            read_hdf_lock.acquire()\n",
    "            self.model.save('{}_historic_b{}_{:.1f}h.h5'.format(self.file_name_prefix, batch, abs_delta_t_h))\n",
    "            read_hdf_lock.release()\n",
    "            self.last_check_time = current_time\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        read_hdf_lock.acquire()\n",
    "        self.model.save('{}_e{}.h5'.format(self.file_name_prefix, epoch))\n",
    "        read_hdf_lock.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_suffix, model_name, data_loader, epoch, n_epochs):\n",
    "\n",
    "    train_name = '%s_%s' % (model_name, train_suffix)  \n",
    "    log_name = \"%s.log\" % train_name\n",
    "    if os.path.isfile(log_name):\n",
    "        close_file(log_name)\n",
    "        os.remove(log_name)\n",
    "    csv_log = CSVLogger(log_name, append=True)\n",
    "\n",
    "    time_checkpoint = TimeCheckpoint(12*60*60, train_name)\n",
    "    callbacks = [time_checkpoint, csv_log]\n",
    "    fit_hist = model.fit_generator(data_loader.generator(True), validation_data=data_loader.generator(False),\n",
    "                                   steps_per_epoch=data_loader.steps_per_epoch, validation_steps=data_loader.validation_steps,\n",
    "                                   callbacks=callbacks, epochs=n_epochs, initial_epoch=epoch, verbose=1)\n",
    "\n",
    "    model.save(\"%s_final.hdf5\" % train_name)\n",
    "    return fit_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72491602 66391602 6100000\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader('N:/tau-ml/tuples-v2-t3/training/part_*.h5', netConf_full, 500, 10000, validation_size=6100000,\n",
    "                    max_queue_size=40, n_passes=-1, return_grid=True)\n",
    "\n",
    "print(loader.total_size, loader.data_size, loader.validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4 1.0 2.0 0.6000000000000001\n",
      "WARNING:tensorflow:From C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\konst\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "TauLosses.SetSFs(1, 2.5, 5, 1.5)\n",
    "print(TauLosses.Le_sf, TauLosses.Lmu_sf, TauLosses.Ltau_sf, TauLosses.Ljet_sf)\n",
    "model_name = \"DeepTau2017v2p5\"\n",
    "model = LoadModel('DeepTau2017v2p5_step1_final.hdf5')\n",
    "compile_model(model, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "132784/132784 [==============================] - 69979s 527ms/step - loss: 0.2815 - acc: 0.8599 - tau_crossentropy: 0.1033 - tau_crossentropy_v2: 0.2777 - Le: 0.0661 - Lmu: 0.0129 - Ljet: 0.1066 - He: 0.1965 - Hmu: 0.0344 - Htau: 0.0721 - Hjet: 0.1793 - Hcat_e: 0.0310 - Hcat_mu: 0.0011 - Hcat_jet: 0.0684 - Hbin: 0.0637 - Hcat_eInv: 0.1654 - Hcat_muInv: 0.0332 - Hcat_jetInv: 0.1110 - Fe: 0.0079 - Fmu: 0.0015 - Fjet: 0.0100 - Fcmb: 0.0402 - weighted_acc: 0.8391 - weighted_tau_crossentropy: 0.1098 - weighted_tau_crossentropy_v2: 0.2815 - weighted_Le: 0.0835 - weighted_Lmu: 0.0235 - weighted_Ljet: 0.0881 - weighted_He: 0.2004 - weighted_Hmu: 0.0587 - weighted_Htau: 0.0885 - weighted_Hjet: 0.1379 - weighted_Hcat_e: 0.0320 - weighted_Hcat_mu: 0.0012 - weighted_Hcat_jet: 0.0403 - weighted_Hbin: 0.0482 - weighted_Hcat_eInv: 0.1684 - weighted_Hcat_muInv: 0.0575 - weighted_Hcat_jetInv: 0.0976 - weighted_Fe: 0.0098 - weighted_Fmu: 0.0016 - weighted_Fjet: 0.0060 - weighted_Fcmb: 0.0296 - val_loss: 0.2181 - val_acc: 0.9328 - val_tau_crossentropy: 0.0914 - val_tau_crossentropy_v2: 0.2333 - val_Le: 0.0570 - val_Lmu: 0.0083 - val_Ljet: 0.1004 - val_He: 0.1679 - val_Hmu: 0.0309 - val_Htau: 0.0590 - val_Hjet: 0.1869 - val_Hcat_e: 0.0289 - val_Hcat_mu: 0.0010 - val_Hcat_jet: 0.0697 - val_Hbin: 0.0638 - val_Hcat_eInv: 0.1390 - val_Hcat_muInv: 0.0299 - val_Hcat_jetInv: 0.1172 - val_Fe: 0.0057 - val_Fmu: 0.0015 - val_Fjet: 0.0070 - val_Fcmb: 0.0377 - val_weighted_acc: 0.9231 - val_weighted_tau_crossentropy: 0.0895 - val_weighted_tau_crossentropy_v2: 0.2181 - val_weighted_Le: 0.0686 - val_weighted_Lmu: 0.0136 - val_weighted_Ljet: 0.0807 - val_weighted_He: 0.1672 - val_weighted_Hmu: 0.0426 - val_weighted_Htau: 0.0700 - val_weighted_Hjet: 0.1478 - val_weighted_Hcat_e: 0.0268 - val_weighted_Hcat_mu: 7.3161e-04 - val_weighted_Hcat_jet: 0.0430 - val_weighted_Hbin: 0.0465 - val_weighted_Hcat_eInv: 0.1404 - val_weighted_Hcat_muInv: 0.0418 - val_weighted_Hcat_jetInv: 0.1048 - val_weighted_Fe: 0.0059 - val_weighted_Fmu: 9.5505e-04 - val_weighted_Fjet: 0.0034 - val_weighted_Fcmb: 0.0256\n",
      "Epoch 2/5\n",
      " 23201/132784 [====>.........................] - ETA: 15:21:13 - loss: 0.2852 - acc: 0.8700 - tau_crossentropy: 0.1047 - tau_crossentropy_v2: 0.2766 - Le: 0.0658 - Lmu: 0.0130 - Ljet: 0.1089 - He: 0.1917 - Hmu: 0.0327 - Htau: 0.0716 - Hjet: 0.1860 - Hcat_e: 0.0307 - Hcat_mu: 0.0011 - Hcat_jet: 0.0723 - Hbin: 0.0646 - Hcat_eInv: 0.1610 - Hcat_muInv: 0.0317 - Hcat_jetInv: 0.1137 - Fe: 0.0076 - Fmu: 0.0015 - Fjet: 0.0101 - Fcmb: 0.0403 - weighted_acc: 0.8448 - weighted_tau_crossentropy: 0.1139 - weighted_tau_crossentropy_v2: 0.2852 - weighted_Le: 0.0867 - weighted_Lmu: 0.0248 - weighted_Ljet: 0.0907 - weighted_He: 0.1991 - weighted_Hmu: 0.0596 - weighted_Htau: 0.0904 - weighted_Hjet: 0.1424 - weighted_Hcat_e: 0.0340 - weighted_Hcat_mu: 0.0012 - weighted_Hcat_jet: 0.0425 - weighted_Hbin: 0.0491 - weighted_Hcat_eInv: 0.1651 - weighted_Hcat_muInv: 0.0584 - weighted_Hcat_jetInv: 0.0999 - weighted_Fe: 0.0096 - weighted_Fmu: 0.0016 - weighted_Fjet: 0.0059 - weighted_Fcmb: 0.0297"
     ]
    }
   ],
   "source": [
    "fit_hist = run_training('step{}'.format(2), model_name, loader, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
